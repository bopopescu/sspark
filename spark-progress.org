* 0 Scala
** Case classes Chapter 15
   DEADLINE: <2015-06-24 Wed>
case. Syntactic boilerplate added.
val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
func.asInstanceOf[Type] casts func to Type

Partial functions, pattern matching. 15.7



* 1 Build 
Download 1.3.1

build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package

from https://spark.apache.org/docs/latest/building-spark.html
Initial compilation requires scala, groovy, maven etc to be downloaded. 

mvn vs sbt?
http://genomegeek.blogspot.com/2014/11/how-to-install-apache-spark-on-mac-os-x.html

sbt/sbt clean assembly 

Could not build the examples again after modification.
SBT downloads everything from the internet all over again, starting from scala.jar

** Ctags
ctags -e -R . 
-e option generates for emacs
s-. defined as find-tag-other-window, M-* too cumbersome. M-,


* 2 Run Tests. Modify few sample programs

SparkPi with no args. Takes a while to start up.

Emacs ag not working "ag not found". PATH problem. Fixed in .emacs. 


* 3 Checkpoint() sample program

In Spark streaming, each time step results in a new set of RDDs being generated. This is coherent with the "immutable" nature of RDDs --- each RDD partition has a deterministic lineage. 
Thus instead of appending RDDs, new RDDs are created for each timestep.
During checkpointing, a full RDD checkpoint implies checkpointing the RDD data for the particular time step.
The checkpointing is periodic. Not every minute, but every n-th time step (as used by the DStreams). This matches the paper's (every 5th step) assertion.

time-step ~= batch interval


* 4 Logging and Debugging. 5 benchmarks


* 5 DAG Scheduler
** TaskSet
private[spark] class TaskSet(
    val tasks: Array[Task[_]],
    val stageId: Int,
    val attempt: Int,
    val priority: Int,
    val properties: Properties) {
    val id: String = stageId + "." + attempt

** Task Submission
Flow:: handleJobSubmitted -> submitStage -> submitMissingTasks -> ... 

SubmitMissingTasks:: is where the actual tasks are spawned from. Not a very appropriate method name.
submitTasks(new TaskSet(tasks.toArray))

TaskSet:: Just a type

submitTasks:: createTaskSetManager ; activeTaskSets ; addTaskSetManager

Flow:: TaskSetManager:resourceOffer->DAGScheduler:handleBeginEvent -> ExecutorAllocationManager:onTaskStart

** Task End

TaskSetManager:handleSuccessfulTask [calls dopCheckpoint] -> handleTaskCompletion

*** Required Checkpointing Information
Already:: already checkpointed or not
job:: obv
stage:: = stageIdToStage.get(task.stageId)
RDD:: = from stage.rdd
Partition:: partition id
location:: Map of partition->location. cacheLocs array has partition location information. use this?

add RDD field to TaskContext  in submitJob


* 6 Instrument Task-End

* 7 Checkpoint RDD glue

 FutureAction.scala
 sc.runJob(rdd, CheckpointRDD.writeToFile[Int](path.toString, broadcastedConf, 1024) _)

runjob takes rdd, action, and list of partitions.

scala method syntax writetofile. def M[]()(){}. Second pair of parenthesis has ctx. 

SparkContext.scala:submitJob , 1469 doCheckpoint after every job!!!! WHY?

clean(function).

=> syntax

RDDCheckpointData.scala:doCheckpoint
  
Every runJob calls RDD checkpointing at the end. In RDDCheckpointData, if an RDD is not marked for checkpointing, it simply returns. Every checkpoint operation is recursive and goes up through the RDD dependency chain. 

RDD.checkpoint (1337) to be called before the job starts executing.

computeOrReadCheckpoint



* 8 Checkpoint Partitions

isCheckpointed -> ispartitionCheckpointed 


* 9 Combine checkpointed partitions into single checkpointed RDD


* 10 Recovery

CacheManager.scala:getOrCompute

rdd.scala:markCheckpointed . When checkpointed, an RDDs dependencies and *partitions* are cleared. The new parent is the checkpointed RDD file.

