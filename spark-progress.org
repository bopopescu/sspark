* 0 Scala
** Case classes Chapter 15
   DEADLINE: <2015-06-24 Wed>
case. Syntactic boilerplate added.
val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
func.asInstanceOf[Type] casts func to Type

Partial functions, pattern matching. 15.7


* 1 Build 
Download 1.3.1

build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package

from https://spark.apache.org/docs/latest/building-spark.html
Initial compilation requires scala, groovy, maven etc to be downloaded. 

mvn vs sbt?
http://genomegeek.blogspot.com/2014/11/how-to-install-apache-spark-on-mac-os-x.html

sbt/sbt clean assembly 

Could not build the examples again after modification.
SBT downloads everything from the internet all over again, starting from scala.jar

** Flow
Make code changes on local. 

** Buildin
g Examples
Drop into the sbt shell
>project examples
>assembly
Why does the non-interactive version of this below not work? aargh.
sbt project examples/ selects the project. Then run sbt assembly-


** Ctags
ctags -e -R . 
-e option generates for emacs
s-. defined as find-tag-other-window, M-* too cumbersome. M-,

sctags compiles, but not runs

** Scala outline popup

* 2 Run Tests. Modify few sample programs

SparkPi with no args. Takes a while to start up.

Emacs ag not working "ag not found". PATH problem. Fixed in .emacs. 


* 3 Checkpoint() sample program

In Spark streaming, each time step results in a new set of RDDs being generated. This is coherent with the "immutable" nature of RDDs --- each RDD partition has a deterministic lineage. 
Thus instead of appending RDDs, new RDDs are created for each timestep.
During checkpointing, a full RDD checkpoint implies checkpointing the RDD data for the particular time step.
The checkpointing is periodic. Not every minute, but every n-th time step (as used by the DStreams). This matches the paper's (every 5th step) assertion.

time-step ~= batch interval


* 4 Jobs
 FutureAction.scala
 sc.runJob(rdd, CheckpointRDD.writeToFile[Int](path.toString, broadcastedConf, 1024) _)

runjob takes rdd, action, and list of partitions.

scala method syntax writetofile. def M[]()(){}. Second pair of parenthesis has ctx. 

SparkContext.scala:submitJob , 1469 doCheckpoint after every job!!!! WHY?


* 5 Tasks
** TaskSet
private[spark] class TaskSet(
    val tasks: Array[Task[_]],
    val stageId: Int,
    val attempt: Int,
    val priority: Int,
    val properties: Properties) {
    val id: String = stageId + "." + attempt

** Task Submission
Flow:: handleJobSubmitted -> submitStage -> submitMissingTasks -> ... 

SubmitMissingTasks:: is where the actual tasks are spawned from. Not a very appropriate method name.
submitTasks(new TaskSet(tasks.toArray))

TaskSet:: Just a type

submitTasks:: createTaskSetManager ; activeTaskSets ; addTaskSetManager

Flow:: TaskSetManager:resourceOffer->DAGScheduler:handleBeginEvent -> ExecutorAllocationManager:onTaskStart

** Task End

TaskSetManager:handleSuccessfulTask [calls dopCheckpoint] -> handleTaskCompletion

*** Required Checkpointing Information
Already:: already checkpointed or not
job:: obv
stage:: = stageIdToStage.get(task.stageId)
RDD:: = from stage.rdd
Partition:: partition id
location:: Map of partition->location. cacheLocs array has partition location information. use this?


add RDD field to TaskContext  in submitJob



* 6 Instrument Task-End

* 7 Checkpointing

** Flow

iterator:: getOrCompute | computeOrReadCheckpoint -> parent.iterator


cacheManager.getOrCompute:: gets/computes *partition*. Reads data located by blockManager.get(rdd,partition)
computeOrReadCheckpoint(split, context):: if(isCheckpointed) parent.iterator else compute


** Actual Checkpointing

RDDCheckpointData:doCheckpoint:: Actually writes to file



  
Every runJob calls RDD checkpointing at the end. In RDDCheckpointData, if an RDD is not marked for checkpointing, it simply returns. Every checkpoint operation is recursive and goes up through the RDD dependency chain. 

RDD.checkpoint (1337) to be called before the job starts executing.

computeOrReadCheckpoint



* 8 Checkpoint Partitions

isCheckpointed -> ispartitionCheckpointed 


* 9 Combine checkpointed partitions into single checkpointed RDD


* 10 Recovery

CacheManager.scala:getOrCompute

rdd.scala:markCheckpointed . When checkpointed, an RDDs dependencies and *partitions* are cleared. The new parent is the checkpointed RDD file.

* Benchmarks
** PageRank
PageRank with full livejournal graph does not run on a single obelix node due to a variety of ut-of-memory conditions. Fixed by sampling the rdd (10%). Take(10000) returns an array, not an RDD.
Single iteration takes about ~3 minutes

* Memory
Memoryblock manager maintains free space information.
Large data sizes cause plethora of out-of-memory conditions. This is bad.
Partitioning, scheduling, etc should carefully consider how much free memory they have, and take that into account when making decisions.

If a partition is too big to fit into memory, data has be spilled on disk. How are users to figure out how large their executors should be? 

** Number of Partitions
For the initial textFile RDD, the defaultMinPartitions = min(parallelism,2). Parallelism is total number of cores

executor.cores important. In standalone mode, each executor gets all cores. With yarn, only one. 
The number of executors is determined by the amount of free memory???

default.parallelism seems to work. No luck with executor.cores


* Issues
- CheckpointRDD is created at the end. RDD's previous parents are forgotten, etc. 
What is needed is to bypass that mechanism. If partcheckpointing is enabled, and if complete RDD has not been checkpointed, then there should be a way to still read the RDD data. 

- Garbage collection of "unreachable" checkpointedRDDs? Some descendent is checkpointed => delete?


- Mark isCheckpointed once all partitions have been saved, plus do the dependency pruning and restore via CheckpointRDD

- shouldCheckpointRDD

- Store Task metrics somewhere convenient. Access required for checkpointing policy
=======

* Resolving the Control-flow DAG question

Confusion: How can the RDD DAG be generated before a job starts in the
presence of control-flow operations (if-else, loops)?

Short Answer: The complete DAG for a Spark program is not known in advance

Explanation: The confusion arises because of Spark's
terminology. Specifically, a Spark program is NOT EQUAL to a job.
Instead, a "Spark Job" is a DAG of RDD transformations, and a Spark
program (which is the actual job) is composed of multiple "Spark
Jobs".  

Statements in a user program are interpreted dispatched to
Spark. There is no static analysis, but lazy evaluation ensures that
all the RDD transformations are submitted to Spark as part of the
"Spark Job" before execution actually starts.

Effect on models: The dynamic DAG means there is no easy way of
determining the total number of RDDs or job-progress.





* TODO
- checkpointing partitions
- recursive checkpointing
- recovery
- array of checkpointed partitions
- cache/block manager
- 
These "Spark Jobs" are submitted to Spark by the Scala interpreter,
and statements are dispatched to Spark

why is rdd.doCheckpoint called in
SparkContext.scala:runJob ???  spark.logLineage set to true

